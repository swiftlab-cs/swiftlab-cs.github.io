[  
  {
    "title": "The Pensieve Paradigm: Stateful Language Models with Learned Memory Management",
    "date": "2026",
    "authors": [
      "Xiaoyuan Liu",
      "Tian Liang",
      "Dongyang Ma",
      "Deyu Zhou",
      "Haitao Mi",
      "Pinjia He",
      "Yan Wang"
    ],
    "venue": "The Fourteenth International Conference on Learning Representations, April 23-27, 2026",
    "venueShort": "ICLR",
    "tags": [],
    "abstract": "In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve—mature databases and retrieval systems, our models inexplicably lack the \"wand\" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manipulate their own state. We equip our model with a suite of tools, such as dynamic indexing, context pruning, and note-taking, and train it to actively manage this loop. By learning to dynamically construct its own context, our model breaks free from the architectural prison of a fixed window. The results are prominent: our state-management approach decouples performance from context window size, delivering strong accuracy and sustainability under extremely long contexts with linear inference cost. We demonstrate this by showing StateLM reliably retrieves a \"needle\" from a 1-million-token haystack, a task far beyond the reach of conventional models. On practical document QA tasks from NovelQA and LongBench, StateLM outperforms strong instruct baselines while using only 1/4 of their active context. An ablation further shows that our curated training pipeline is more effective for learning memory management than agent-like prompting. Together, these results mark a shift from passive predictors to state-aware systems where reasoning becomes a stateful and manageable process.",
    "projectUrl": null,
    "paperUrl": "https://openreview.net/pdf?id=GymjF88oGQ",
    "slidesUrl": null,
    "bibtex": "",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards",
    "date": "2025",
    "authors": [
      "Xiaoyuan Liu",
      "Tian Liang",
      "Zhiwei He",
      "Jiahao Xu",
      "Wenxuan Wang",
      "Pinjia He",
      "Zhaopeng Tu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "venue": "Annual Conference on Neural Information Processing Systems",
    "venueShort": "NeurIPS",
    "tags": [],
    "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement strategy. However, a prevalent issue is ``superficial self-reflection'', where models fail to robustly verify their own outputs. We introduce RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework designed to tackle this. RISE explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities within a single, integrated RL process. The core mechanism involves leveraging verifiable rewards from an outcome verifier to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions, then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. Extensive experiments on diverse mathematical reasoning benchmarks show that RISE consistently improves model's problem-solving accuracy while concurrently fostering strong self-verification skills. Our analyses highlight the advantages of online verification and the benefits of increased verification compute. Additionally, RISE models exhibit more frequent and accurate self-verification behaviors during reasoning. These advantages reinforce RISE as a flexible and effective path towards developing more robust and self-aware reasoners.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.48550/arXiv.2505.13445",
    "slidesUrl": null,
    "bibtex": "@article{DBLP:journals/corr/abs-2505-13445,\n  author       = {Xiaoyuan Liu and\n                  Tian Liang and\n                  Zhiwei He and\n                  Jiahao Xu and\n                  Wenxuan Wang and\n                  Pinjia He and\n                  Zhaopeng Tu and\n                  Haitao Mi and\n                  Dong Yu},\n  title        = {Trust, But Verify: {A} Self-Verification Approach to Reinforcement\n                  Learning with Verifiable Rewards},\n  journal      = {CoRR},\n  volume       = {abs/2505.13445},\n  year         = {2025},\n  url          = {https://doi.org/10.48550/arXiv.2505.13445},\n  doi          = {10.48550/ARXIV.2505.13445},\n  eprinttype    = {arXiv},\n  eprint       = {2505.13445},\n  timestamp    = {Wed, 25 Jun 2025 08:29:06 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-2505-13445.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": "https://arxiv.org/abs/2505.13445",
    "awards": []
  },
  {
    "title": "Towards Evaluating Proactive Risk Awareness of Multimodal Language Models",
    "date": "2025",
    "authors": [
      "Youliang Yuan",
      "Wenxiang Jiao",
      "Yuejin Xie",
      "Chihao Shen",
      "Menghan Tian",
      "Wenxuan Wang",
      "Jen-tse Huang",
      "Pinjia He"
    ],
    "venue": "Annual Conference on Neural Information Processing Systems",
    "venueShort": "NeurIPS",
    "tags": [],
    "abstract": "Human safety awareness gaps often prevent the timely recognition of everyday risks. In solving this problem, a proactive safety artificial intelligence (AI) system would work better than a reactive one. Instead of just reacting to users' questions, it would actively watch people's behavior and their environment to detect potential dangers in advance. Our Proactive Safety Bench (PaSBench) evaluates this capability through 416 multimodal scenarios (128 image sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation of 36 advanced models reveals fundamental limitations: Top performers like Gemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks in repeated trials. Through failure analysis, we identify unstable proactive reasoning rather than knowledge deficits as the primary limitation. This work establishes (1) a proactive safety benchmark, (2) systematic evidence of model limitations, and (3) critical directions for developing reliable protective AI. We believe our dataset and findings can promote the development of safer AI assistants that actively prevent harm rather than merely respond to requests. Our dataset can be found at https://huggingface.co/datasets/Youliang/PaSBench.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.48550/arXiv.2505.17455",
    "slidesUrl": null,
    "bibtex": "@article{DBLP:journals/corr/abs-2505-17455,\n  author       = {Youliang Yuan and\n                  Wenxiang Jiao and\n                  Yuejin Xie and\n                  Chihao Shen and\n                  Menghan Tian and\n                  Wenxuan Wang and\n                  Jen{-}tse Huang and\n                  Pinjia He},\n  title        = {Towards Evaluating Proactive Risk Awareness of Multimodal Language\n                  Models},\n  journal      = {CoRR},\n  volume       = {abs/2505.17455},\n  year         = {2025},\n  url          = {https://doi.org/10.48550/arXiv.2505.17455},\n  doi          = {10.48550/ARXIV.2505.17455},\n  eprinttype    = {arXiv},\n  eprint       = {2505.17455},\n  timestamp    = {Thu, 26 Jun 2025 21:04:08 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-2505-17455.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": "https://arxiv.org/abs/2505.17455",
    "awards": []
  },
  {
    "title": "OpenRCA: Can Large Language Models Locate the Root Cause of Software Failures?",
    "date": "2025",
    "authors": [
      "Junjielong Xu",
      "Qinan Zhang",
      "Zhiqing Zhong",
      "Shilin He",
      "Chaoyun Zhang",
      "Qingwei Lin",
      "Dan Pei",
      "Pinjia He",
      "Dongmei Zhang",
      "Qi Zhang"
    ],
    "venue": "The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025",
    "venueShort": "ICLR",
    "tags": [],
    "abstract": "Large language models (LLMs) are driving substantial advancements in software engineering, with successful applications like Copilot and Cursor transforming real-world development practices. However, current research predominantly focuses on the early stages of development, such as code generation, while overlooking the post-development phases that are crucial to user experience. To explore the potential of LLMs in this direction, we propose OpenRCA, a benchmark dataset and evaluation framework for assessing LLMs’ ability to identify the root cause of software failures. OpenRCA includes 335 failures from three enterprise software systems, along with over 68 GB of telemetry data (logs, metrics, and traces). Given a failure case and its associated telemetry, the LLM is tasked to identify the root cause that triggered the failure, requiring comprehension of software dependencies and reasoning over heterogeneous, long-context telemetry data. Our results show substantial room for improvement, as current models can only handle the simplest cases. Even with the specially designed RCA-agent, the best-performing model, Claude 3.5, solved only 11.34% failure cases. Our work paves the way for future research in this direction.",
    "projectUrl": null,
    "paperUrl": "https://openreview.net/forum?id=M4qNIzQYpd",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/iclr/XuZZHZLPHZ025,\n  author       = {Junjielong Xu and\n                  Qinan Zhang and\n                  Zhiqing Zhong and\n                  Shilin He and\n                  Chaoyun Zhang and\n                  Qingwei Lin and\n                  Dan Pei and\n                  Pinjia He and\n                  Dongmei Zhang and\n                  Qi Zhang},\n  title        = {OpenRCA: Can Large Language Models Locate the Root Cause of Software\n                  Failures?},\n  booktitle    = {The Thirteenth International Conference on Learning Representations,\n                  {ICLR} 2025, Singapore, April 24-28, 2025},\n  publisher    = {OpenReview.net},\n  year         = {2025},\n  url          = {https://openreview.net/forum?id=M4qNIzQYpd},\n  timestamp    = {Thu, 15 May 2025 17:19:05 +0200},\n  biburl       = {https://dblp.org/rec/conf/iclr/XuZZHZLPHZ025.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Aligning the Objective of LLM-Based Program Repair",
    "date": "2025",
    "authors": [
      "Junjielong Xu",
      "Ying Fu",
      "Shin Hwei Tan",
      "Pinjia He"
    ],
    "venue": "47th IEEE/ACM International Conference on Software Engineering, ICSE 2025, Ottawa, ON, Canada, April 26 - May 6, 2025",
    "venueShort": "ICSE",
    "tags": [],
    "abstract": "Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10% and reduces the patch sampling number by 90%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1109/ICSE55347.2025.00169",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/icse/XuFTH25,\n  author       = {Junjielong Xu and\n                  Ying Fu and\n                  Shin Hwei Tan and\n                  Pinjia He},\n  title        = {Aligning the Objective of LLM-Based Program Repair},\n  booktitle    = {47th {IEEE/ACM} International Conference on Software Engineering,\n                  {ICSE} 2025, Ottawa, ON, Canada, April 26 - May 6, 2025},\n  pages        = {2548--2560},\n  publisher    = {{IEEE}},\n  year         = {2025},\n  url          = {https://doi.org/10.1109/ICSE55347.2025.00169},\n  doi          = {10.1109/ICSE55347.2025.00169},\n  timestamp    = {Mon, 30 Jun 2025 13:02:20 +0200},\n  biburl       = {https://dblp.org/rec/conf/icse/XuFTH25.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "An Empirical Study on Package-Level Deprecation in Python Ecosystem",
    "date": "2025",
    "authors": [
      "Zhiqing Zhong",
      "Shilin He",
      "Haoxuan Wang",
      "Boxi Yu",
      "Haowen Yang",
      "Pinjia He"
    ],
    "venue": "47th IEEE/ACM International Conference on Software Engineering, ICSE 2025, Ottawa, ON, Canada, April 26 - May 6, 2025",
    "venueShort": "ICSE",
    "tags": [],
    "abstract": "Open-source software (OSS) plays a crucial role in modern software development. Utilizing OSS code can greatly accelerate software development, reduce redundancy, and enhance reliability. Python, a widely adopted programming language, is renowned for its extensive and diverse third-party package ecosystem. However, a significant number of OSS packages within the Python ecosystem are in poor maintenance, leading to potential risks in functionality and security. Consequently, it is essential to establish a deprecation mechanism to assist package developers and users in managing packages effectively.\n  To facilitate the establishment of the package-level deprecation mechanism, this paper presents a mixed-method empirical study, including data analysis and surveys. We investigate the current practices of announcing, receiving, and handling package-level deprecation in the Python ecosystem. We also assess the benefits of having deprecation announcements for inactively maintained packages. Furthermore, we investigate the challenges faced by package developers and users and their expectations for future deprecation practices. Our findings reveal that 75.4% of inactive package developers have no intention of releasing deprecation declarations for various reasons, while 89.5% of users express a desire to be notified about the deprecation, highlighting a gap between developers and users; in many cases, no alternative solutions are available when deprecation occurs, emphasizing the need to explore practical approaches that enable seamless package handover and require less maintenance effort. Our work aims to enhance the understanding of existing package-level deprecation patterns within the Python OSS realm and facilitate the development of deprecation practices for the Python community in the future.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1109/ICSE55347.2025.00046",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/icse/ZhongHWYYH25,\n  author       = {Zhiqing Zhong and\n                  Shilin He and\n                  Haoxuan Wang and\n                  Boxi Yu and\n                  Haowen Yang and\n                  Pinjia He},\n  title        = {An Empirical Study on Package-Level Deprecation in Python Ecosystem},\n  booktitle    = {47th {IEEE/ACM} International Conference on Software Engineering,\n                  {ICSE} 2025, Ottawa, ON, Canada, April 26 - May 6, 2025},\n  pages        = {66--77},\n  publisher    = {{IEEE}},\n  year         = {2025},\n  url          = {https://doi.org/10.1109/ICSE55347.2025.00046},\n  doi          = {10.1109/ICSE55347.2025.00046},\n  timestamp    = {Sun, 07 Dec 2025 22:11:09 +0100},\n  biburl       = {https://dblp.org/rec/conf/icse/ZhongHWYYH25.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training",
    "date": "2025",
    "authors": [
      "Youliang Yuan",
      "Wenxiang Jiao",
      "Wenxuan Wang",
      "Jen-tse Huang",
      "Jiahao Xu",
      "Tian Liang",
      "Pinjia He",
      "Zhaopeng Tu"
    ],
    "venue": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025",
    "venueShort": "ACL",
    "tags": [],
    "abstract": "This study addresses a critical gap in safety tuning practices for Large Language Models (LLMs) by identifying and tackling a refusal position bias within safety tuning data, which compromises the models' ability to appropriately refuse generating unsafe content. We introduce a novel approach, Decoupled Refusal Training (DeRTa), designed to empower LLMs to refuse compliance to harmful prompts at any response position, significantly enhancing their safety capabilities. DeRTa incorporates two novel components: (1) Maximum Likelihood Estimation (MLE) with Harmful Response Prefix, which trains models to recognize and avoid unsafe content by appending a segment of harmful response to the beginning of a safe response, and (2) Reinforced Transition Optimization (RTO), which equips models with the ability to transition from potential harm to safety refusal consistently throughout the harmful response sequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model families across six attack scenarios, demonstrates that our method not only improves model safety without compromising performance but also surpasses baseline methods in defending against attacks.",
    "projectUrl": null,
    "paperUrl": "https://aclanthology.org/2025.acl-long.158/",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/acl/YuanJ00XLHT25,\n  author       = {Youliang Yuan and\n                  Wenxiang Jiao and\n                  Wenxuan Wang and\n                  Jen{-}tse Huang and\n                  Jiahao Xu and\n                  Tian Liang and\n                  Pinjia He and\n                  Zhaopeng Tu},\n  editor       = {Wanxiang Che and\n                  Joyce Nabende and\n                  Ekaterina Shutova and\n                  Mohammad Taher Pilehvar},\n  title        = {Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled\n                  Refusal Training},\n  booktitle    = {Proceedings of the 63rd Annual Meeting of the Association for Computational\n                  Linguistics (Volume 1: Long Papers), {ACL} 2025, Vienna, Austria,\n                  July 27 - August 1, 2025},\n  pages        = {3149--3167},\n  publisher    = {Association for Computational Linguistics},\n  year         = {2025},\n  url          = {https://aclanthology.org/2025.acl-long.158/},\n  timestamp    = {Sun, 02 Nov 2025 21:27:24 +0100},\n  biburl       = {https://dblp.org/rec/conf/acl/YuanJ00XLHT25.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench",
    "date": "2025",
    "authors": [
      "Boxi Yu",
      "Yuxuan Zhu",
      "Pinjia He",
      "Daniel Kang"
    ],
    "venue": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025",
    "venueShort": "ACL",
    "tags": [],
    "abstract": "The advent of Large Language Models (LLMs) has spurred the development of coding agents for real-world code generation. As a widely used benchmark for evaluating the code generation capabilities of these agents, SWE-Bench uses real-world problems based on GitHub issues and their corresponding pull requests. However, the manually written test cases included in these pull requests are often insufficient, allowing generated patches to pass the tests without resolving the underlying issue. To address this challenge, we introduce UTGenerator, an LLM-driven test case generator that automatically analyzes codebases and dependencies to generate test cases for real-world Python projects. Building on UTGenerator, we propose UTBoost, a comprehensive framework for test case augmentation. In our evaluation, we identified 36 task instances with insufficient test cases and uncovered 345 erroneous patches incorrectly labeled as passed in the original SWE Bench. These corrections, impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard entries, yield 18 and 11 ranking changes, respectively.",
    "projectUrl": null,
    "paperUrl": "https://aclanthology.org/2025.acl-long.189/",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/acl/YuZHK25,\n  author       = {Boxi Yu and\n                  Yuxuan Zhu and\n                  Pinjia He and\n                  Daniel Kang},\n  editor       = {Wanxiang Che and\n                  Joyce Nabende and\n                  Ekaterina Shutova and\n                  Mohammad Taher Pilehvar},\n  title        = {UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench},\n  booktitle    = {Proceedings of the 63rd Annual Meeting of the Association for Computational\n                  Linguistics (Volume 1: Long Papers), {ACL} 2025, Vienna, Austria,\n                  July 27 - August 1, 2025},\n  pages        = {3762--3774},\n  publisher    = {Association for Computational Linguistics},\n  year         = {2025},\n  url          = {https://aclanthology.org/2025.acl-long.189/},\n  timestamp    = {Wed, 28 Jan 2026 14:54:15 +0100},\n  biburl       = {https://dblp.org/rec/conf/acl/YuZHK25.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "A Memory-Disaggregated Radix Tree",
    "date": "2024",
    "authors": [
      "Xuchuan Luo",
      "Pengfei Zuo",
      "Jiacheng Shen",
      "Jiazhen Gu",
      "Xin Wang",
      "Michael Rung-Tsong Lyu",
      "Yangfan Zhou"
    ],
    "venue": "ACM Trans. Storage",
    "venueShort": "TOS",
    "tags": [],
    "abstract": "Disaggregated memory (DM) is an increasingly prevalent architecture with high resource utilization. It separates computing and memory resources into two pools and interconnects them with fast networks. Existing range indexes on DM are based on B+ trees, which suffer from large inherent read and write amplifications. The read and write amplifications rapidly saturate the network bandwidth, resulting in low request throughput and high access latency of B+ trees on DM. In this article, we propose that the radix tree is more suitable for DM than the B+ tree due to smaller read and write amplifications. However, constructing a radix tree on DM is challenging due to the costly lock-based concurrency control, the bounded memory-side IOPS, and the complicated computing-side cache validation. To address these challenges, we design SMART, the first radix tree for disaggregated memory with high performance. Specifically, we leverage (1) a hybrid concurrency control scheme including lock-free internal nodes and fine-grained lock-based leaf nodes to reduce lock overhead, (2) a computing-side read-delegation and write-combining technique to break through the IOPS upper bound by reducing redundant I/Os, and (3) a simple yet effective reverse check mechanism for computing-side cache validation. Experimental results show that SMART achieves 6.1× higher throughput under typical write-intensive workloads and 2.8× higher throughput under read-only workloads in YCSB benchmarks, compared with state-of-the-art B+ trees on DM.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1145/3664289",
    "slidesUrl": null,
    "bibtex": "@article{DBLP:journals/tos/LuoZSGWLZ24,\n  author       = {Xuchuan Luo and\n                  Pengfei Zuo and\n                  Jiacheng Shen and\n                  Jiazhen Gu and\n                  Xin Wang and\n                  Michael Rung{-}Tsong Lyu and\n                  Yangfan Zhou},\n  title        = {A Memory-Disaggregated Radix Tree},\n  journal      = {{ACM} Trans. Storage},\n  volume       = {20},\n  number       = {3},\n  pages        = {15},\n  year         = {2024},\n  url          = {https://doi.org/10.1145/3664289},\n  doi          = {10.1145/3664289},\n  timestamp    = {Mon, 10 Nov 2025 08:59:24 +0100},\n  biburl       = {https://dblp.org/rec/journals/tos/LuoZSGWLZ24.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "CHIME: A Cache-Efficient and High-Performance Hybrid Index on Disaggregated Memory",
    "date": "2024",
    "authors": [
      "Xuchuan Luo",
      "Jiacheng Shen",
      "Pengfei Zuo",
      "Xin Wang",
      "Michael R. Lyu",
      "Yangfan Zhou"
    ],
    "venue": "Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles, SOSP 2024, Austin, TX, USA, November 4-6, 2024",
    "venueShort": "SOSP",
    "tags": [],
    "abstract": "Disaggregated memory (DM) is a widely discussed datacenter architecture in academia and industry. It decouples computing and memory resources from monolithic servers into two network-connected resource pools. Range indexes are widely adopted by storage systems on DM to efficiently locate and query remote data. However, existing range indexes on DM suffer from either high computing-side cache consumption or high memory-side read amplifications. In this paper, we propose CHIME, a hybrid index combining B+ trees with hopscotch hashing, to achieve low cache consumption and low read amplifications simultaneously. There are three challenges in constructing CHIME on DM, i.e., the complicated optimistic synchronization, the extra metadata access, and the read amplifications introduced by hopscotch hashing. CHIME leverages 1) a three-level optimistic synchronization scheme to synchronize read and write operations with various granularities, 2) an access-aggregated metadata management technique to eliminate extra metadata accesses by piggybacking and replicating metadata, and 3) an effective hotness-aware speculative read mechanism to mitigate the read amplifications of hopscotch hashing. Experimental results show that CHIME outperforms the state-of-the-art range indexes on DM by up to 5.1× with the same cache size and achieves similar performance with up to 8.7× lower cache consumption.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1145/3694715.3695959",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/sosp/LuoSZ0LZ24,\n  author       = {Xuchuan Luo and\n                  Jiacheng Shen and\n                  Pengfei Zuo and\n                  Xin Wang and\n                  Michael R. Lyu and\n                  Yangfan Zhou},\n  editor       = {Emmett Witchel and\n                  Christopher J. Rossbach and\n                  Andrea C. Arpaci{-}Dusseau and\n                  Kimberly Keeton},\n  title        = {{CHIME:} {A} Cache-Efficient and High-Performance Hybrid Index on\n                  Disaggregated Memory},\n  booktitle    = {Proceedings of the {ACM} {SIGOPS} 30th Symposium on Operating Systems\n                  Principles, {SOSP} 2024, Austin, TX, USA, November 4-6, 2024},\n  pages        = {110--126},\n  publisher    = {{ACM}},\n  year         = {2024},\n  url          = {https://doi.org/10.1145/3694715.3695959},\n  doi          = {10.1145/3694715.3695959},\n  timestamp    = {Mon, 10 Nov 2025 08:59:24 +0100},\n  biburl       = {https://dblp.org/rec/conf/sosp/LuoSZ0LZ24.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Deep Learning or Classical Machine Learning? An Empirical Study on Log-Based Anomaly Detection",
    "date": "2024",
    "authors": [
      "Boxi Yu",
      "Jiayi Yao",
      "Qiuai Fu",
      "Zhiqing Zhong",
      "Haotian Xie",
      "Yaoliang Wu",
      "Yuchi Ma",
      "Pinjia He"
    ],
    "venue": "Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, ICSE 2024, Lisbon, Portugal, April 14-20, 2024",
    "venueShort": "ICSE",
    "tags": [],
    "abstract": "While deep learning (DL) has emerged as a powerful technique, its benefits must be carefully considered in relation to computational costs. Specifically, although DL methods have achieved strong performance in log anomaly detection, they often require extended time for log preprocessing, model training, and model inference, hindering their adoption in online distributed cloud systems that require rapid deployment of log anomaly detection service. This paper investigates the superiority of DL methods compared to simpler techniques in log anomaly detection. We evaluate basic algorithms (e.g., KNN, SLFN) and DL approaches (e.g., CNN) on five public log anomaly detection datasets (e.g., HDFS). Our findings demonstrate that simple algorithms outperform DL methods in both time efficiency and accuracy. For instance, on the Thunderbird dataset, the K-nearest neighbor algorithm trains 1,000 times faster than NeuralLog while achieving a higher F1-Score by 0.0625. We also identify three factors contributing to this phenomenon, which are: (1) redundant log preprocessing strategies, (2) dataset simplicity, and (3) the nature of binary classification in log anomaly detection. To assess the necessity of DL, we propose LightAD, an architecture that optimizes training time, inference time, and performance score. With automated hyper-parameter tuning, LightAD allows fair comparisons among log anomaly detection models, enabling engineers to evaluate the suitability of complex DL methods. Our findings serve as a cautionary tale for the log anomaly detection community, highlighting the need to critically analyze datasets and research tasks before adopting DL approaches. Researchers proposing computationally expensive models should benchmark their work against lightweight algorithms to ensure a comprehensive evaluation.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1145/3597503.3623308",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/icse/YuYFZXWMH24,\n  author       = {Boxi Yu and\n                  Jiayi Yao and\n                  Qiuai Fu and\n                  Zhiqing Zhong and\n                  Haotian Xie and\n                  Yaoliang Wu and\n                  Yuchi Ma and\n                  Pinjia He},\n  title        = {Deep Learning or Classical Machine Learning? An Empirical Study on\n                  Log-Based Anomaly Detection},\n  booktitle    = {Proceedings of the 46th {IEEE/ACM} International Conference on Software\n                  Engineering, {ICSE} 2024, Lisbon, Portugal, April 14-20, 2024},\n  pages        = {35:1--35:13},\n  publisher    = {{ACM}},\n  year         = {2024},\n  url          = {https://doi.org/10.1145/3597503.3623308},\n  doi          = {10.1145/3597503.3623308},\n  timestamp    = {Sun, 19 Jan 2025 13:14:51 +0100},\n  biburl       = {https://dblp.org/rec/conf/icse/YuYFZXWMH24.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
    "date": "2024",
    "authors": [
      "Youliang Yuan",
      "Wenxiang Jiao",
      "Wenxuan Wang",
      "Jen-tse Huang",
      "Pinjia He",
      "Shuming Shi",
      "Zhaopeng Tu"
    ],
    "venue": "The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024",
    "venueShort": "ICLR",
    "tags": [],
    "abstract": "Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a ''secret cipher'', and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. Our code and data will be released at https://github.com/RobustNLP/CipherChat.",
    "projectUrl": null,
    "paperUrl": "https://openreview.net/forum?id=MbfAK4s61A",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/iclr/YuanJW0H0T24,\n  author       = {Youliang Yuan and\n                  Wenxiang Jiao and\n                  Wenxuan Wang and\n                  Jen{-}tse Huang and\n                  Pinjia He and\n                  Shuming Shi and\n                  Zhaopeng Tu},\n  title        = {{GPT-4} Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher},\n  booktitle    = {The Twelfth International Conference on Learning Representations,\n                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},\n  publisher    = {OpenReview.net},\n  year         = {2024},\n  url          = {https://openreview.net/forum?id=MbfAK4s61A},\n  timestamp    = {Thu, 08 Aug 2024 12:56:51 +0200},\n  biburl       = {https://dblp.org/rec/conf/iclr/YuanJW0H0T24.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "MicroRes: Versatile Resilience Profiling in Microservices via Degradation Dissemination Indexing",
    "date": "2024",
    "authors": [
      "Tianyi Yang",
      "Cheryl Lee",
      "Jiacheng Shen",
      "Yuxin Su",
      "Cong Feng",
      "Yongqiang Yang",
      "Michael R. Lyu"
    ],
    "venue": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2024, Vienna, Austria, September 16-20, 2024",
    "venueShort": "ISSTA",
    "tags": [],
    "abstract": "Microservice resilience, the ability of microservices to recover from failures and continue providing reliable and responsive services, is crucial for cloud vendors. However, the current practice relies on manually configured rules specific to a certain microservice system, resulting in labor-intensity and flexibility issues, given the large scale and high dynamics of microservices. A more labor-efficient and versatile solution is desired. Our insight is that resilient deployment can effectively prevent the dissemination of degradation from system performance metrics to user-aware metrics, and the latter directly affects service quality. In other words, failures in a non-resilient deployment can impact both types of metrics, leading to user dissatisfaction. With this in mind, we propose MicroRes, the first versatile resilience profiling framework for microservices via degradation dissemination indexing. MicroRes first injects failures into microservices and collects available monitoring metrics. Then, it ranks the metrics according to their contributions to the overall service degradation. It produces a resilience index by how much the degradation is disseminated from system performance metrics to user-aware metrics. Higher degradation dissemination indicates lower resilience. We evaluate MicroRes on two open-source and one industrial microservice system. The experiments show MicroRes' efficient and effective resilience profiling of microservices. We also showcase MicroRes' practical usage in production.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1145/3650212.3652131",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/issta/YangLS0FYL24,\n  author       = {Tianyi Yang and\n                  Cheryl Lee and\n                  Jiacheng Shen and\n                  Yuxin Su and\n                  Cong Feng and\n                  Yongqiang Yang and\n                  Michael R. Lyu},\n  editor       = {Maria Christakis and\n                  Michael Pradel},\n  title        = {MicroRes: Versatile Resilience Profiling in Microservices via Degradation\n                  Dissemination Indexing},\n  booktitle    = {Proceedings of the 33rd {ACM} {SIGSOFT} International Symposium on\n                  Software Testing and Analysis, {ISSTA} 2024, Vienna, Austria, September\n                  16-20, 2024},\n  pages        = {325--337},\n  publisher    = {{ACM}},\n  year         = {2024},\n  url          = {https://doi.org/10.1145/3650212.3652131},\n  doi          = {10.1145/3650212.3652131},\n  timestamp    = {Sun, 19 Jan 2025 13:25:56 +0100},\n  biburl       = {https://dblp.org/rec/conf/issta/YangLS0FYL24.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Testing Graph Database Systems via Equivalent Query Rewriting",
    "date": "2024",
    "authors": [
      "Qiuyang Mang",
      "Aoyang Fang",
      "Boxi Yu",
      "Hanfei Chen",
      "Pinjia He"
    ],
    "venue": "Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, ICSE 2024, Lisbon, Portugal, April 14-20, 2024",
    "venueShort": "ICSE",
    "tags": [],
    "abstract": "Graph Database Management Systems (GDBMS), which utilize graph models for data storage and execute queries via graph traversals, have seen ubiquitous usage in real-world scenarios such as recommendation systems, knowledge graphs, and social networks. Much like Relational Database Management Systems (RDBMS), GDBMS are not immune to bugs. These bugs typically manifest as logic errors that yield incorrect results (e.g., omitting a node that should be included), performance bugs (e.g., long execution time caused by redundant graph scanning), and exception issues (e.g., unexpected or missing exceptions). This paper adapts Equivalent Query Rewriting (EQR) to GDBMS testing. EQR rewrites a GDBMS query into equivalent ones that trigger distinct query plans, and checks whether they exhibit discrepancies in system behaviors. To facilitate the realization of EQR, we propose a general concept called Abstract Syntax Graph (ASG). Its core idea is to embed the semantics of a base query into the paths of a graph, which can be utilized to generate new queries with customized properties (e.g., equivalence). Given a base query, an ASG is constructed and then an equivalent query can be generated by finding paths collectively carrying the complete semantics of the base query. To this end, we further design Random Walk Covering (RWC), a simple yet effective path covering algorithm. As a practical implementation of these ideas, we develop a tool GRev, which has successfully detected 22 previously unknown bugs across 5 popular GDBMS, with 15 of them being confirmed. In particular, 14 of the detected bugs are related to improper implementation of graph data retrieval in GDBMS, which is challenging to identify for existing techniques.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1145/3597503.3639200",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/icse/MangFYCH24,\n  author       = {Qiuyang Mang and\n                  Aoyang Fang and\n                  Boxi Yu and\n                  Hanfei Chen and\n                  Pinjia He},\n  title        = {Testing Graph Database Systems via Equivalent Query Rewriting},\n  booktitle    = {Proceedings of the 46th {IEEE/ACM} International Conference on Software\n                  Engineering, {ICSE} 2024, Lisbon, Portugal, April 14-20, 2024},\n  pages        = {143:1--143:12},\n  publisher    = {{ACM}},\n  year         = {2024},\n  url          = {https://doi.org/10.1145/3597503.3639200},\n  doi          = {10.1145/3597503.3639200},\n  timestamp    = {Sun, 19 Jan 2025 13:14:48 +0100},\n  biburl       = {https://dblp.org/rec/conf/icse/MangFYCH24.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Ditto: An Elastic and Adaptive Memory-Disaggregated Caching System",
    "date": "2023",
    "authors": [
      "Jiacheng Shen",
      "Pengfei Zuo",
      "Xuchuan Luo",
      "Yuxin Su",
      "Jiazhen Gu",
      "Hao Feng",
      "Yangfan Zhou",
      "Michael R. Lyu"
    ],
    "venue": "Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023",
    "venueShort": "SOSP",
    "tags": [],
    "abstract": "In-memory caching systems are fundamental building blocks in cloud services. However, due to the coupled CPU and memory on monolithic servers, existing caching systems cannot elastically adjust resources in a resource-efficient and agile manner. To achieve better elasticity, we propose to port inmemory caching systems to the disaggregated memory (DM) architecture, where compute and memory resources are decoupled and can be allocated flexibly. However, constructing an elastic caching system on DM is challenging since accessing cached objects with CPU-bypass remote memory accesses hinders the execution of caching algorithms. Moreover, the elastic changes of compute and memory resources on DM affect the access patterns of cached data, compromising the hit rates of caching algorithms. We design Ditto, the first caching system on DM, to address these challenges. Ditto first proposes a client-centric caching framework to efficiently execute various caching algorithms in the compute pool of DM, relying only on remote memory accesses. Then, Ditto employs a distributed adaptive caching scheme that adaptively switches to the best-fit caching algorithm in real-time based on the performance of multiple caching algorithms to improve cache hit rates. Our experiments show that Ditto effectively adapts to the changing resources on DM and outperforms the state-of-the-art caching systems by up to 3.6× in real-world workloads and 9× in YCSB benchmarks.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1145/3600006.3613144",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/sosp/ShenZL0GFZL23,\n  author       = {Jiacheng Shen and\n                  Pengfei Zuo and\n                  Xuchuan Luo and\n                  Yuxin Su and\n                  Jiazhen Gu and\n                  Hao Feng and\n                  Yangfan Zhou and\n                  Michael R. Lyu},\n  editor       = {Jason Flinn and\n                  Margo I. Seltzer and\n                  Peter Druschel and\n                  Antoine Kaufmann and\n                  Jonathan Mace},\n  title        = {Ditto: An Elastic and Adaptive Memory-Disaggregated Caching System},\n  booktitle    = {Proceedings of the 29th Symposium on Operating Systems Principles,\n                  {SOSP} 2023, Koblenz, Germany, October 23-26, 2023},\n  pages        = {675--691},\n  publisher    = {{ACM}},\n  year         = {2023},\n  url          = {https://doi.org/10.1145/3600006.3613144},\n  doi          = {10.1145/3600006.3613144},\n  timestamp    = {Mon, 10 Nov 2025 08:59:24 +0100},\n  biburl       = {https://dblp.org/rec/conf/sosp/ShenZL0GFZL23.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "FUSEE: A Fully Memory-Disaggregated Key-Value Store",
    "date": "2023",
    "authors": [
      "Jiacheng Shen",
      "Pengfei Zuo",
      "Xuchuan Luo",
      "Tianyi Yang",
      "Yuxin Su",
      "Yangfan Zhou",
      "Michael R. Lyu"
    ],
    "venue": "21st USENIX Conference on File and Storage Technologies, FAST 2023, Santa Clara, CA, USA, February 21-23, 2023",
    "venueShort": "FAST",
    "tags": [],
    "abstract": "Distributed in-memory key-value (KV) stores are embracing the disaggregated memory (DM) architecture for higher resource utilization. However, existing KV stores on DM employ a semi-disaggregated design that stores KV pairs on DM but manages metadata with monolithic metadata servers, hence still suffering from low resource efficiency on metadata servers. To address this issue, this paper proposes FUSEE, a FUlly memory-diSaggrEgated KV StorE that brings disaggregation to metadata management. FUSEE replicates metadata, i.e., the index and memory management information, on memory nodes, manages them directly on the client side, and handles complex failures under the DM architecture. To scalably replicate the index on clients, FUSEE proposes a client-centric replication protocol that allows clients to concurrently access and modify the replicated index. To efficiently manage disaggregated memory, FUSEE adopts a two-level memory management scheme that splits the memory management duty among clients and memory nodes. Finally, to handle the metadata corruption under client failures, FUSEE leverages an embedded operation log scheme to repair metadata with low log maintenance overhead. We evaluate FUSEE with both micro and YCSB hybrid benchmarks. The experimental results show that FUSEE outperforms the state-of-the-art KV stores on DM by up to 4.5 times with less resource consumption.",
    "projectUrl": null,
    "paperUrl": "https://www.usenix.org/conference/fast23/presentation/shen",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/fast/ShenZLY0ZL23,\n  author       = {Jiacheng Shen and\n                  Pengfei Zuo and\n                  Xuchuan Luo and\n                  Tianyi Yang and\n                  Yuxin Su and\n                  Yangfan Zhou and\n                  Michael R. Lyu},\n  editor       = {Ashvin Goel and\n                  Dalit Naor},\n  title        = {{FUSEE:} {A} Fully Memory-Disaggregated Key-Value Store},\n  booktitle    = {21st {USENIX} Conference on File and Storage Technologies, {FAST}\n                  2023, Santa Clara, CA, USA, February 21-23, 2023},\n  pages        = {81--98},\n  publisher    = {{USENIX} Association},\n  year         = {2023},\n  url          = {https://www.usenix.org/conference/fast23/presentation/shen},\n  timestamp    = {Tue, 11 Nov 2025 08:43:30 +0100},\n  biburl       = {https://dblp.org/rec/conf/fast/ShenZLY0ZL23.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "ROME: Testing Image Captioning Systems via Recursive Object Melting",
    "date": "2023",
    "authors": [
      "Boxi Yu",
      "Zhiqing Zhong",
      "Jiaqi Li",
      "Yixing Yang",
      "Shilin He",
      "Pinjia He"
    ],
    "venue": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2023, Seattle, WA, USA, July 17-21, 2023",
    "venueShort": "ISSTA",
    "tags": [],
    "abstract": "Image captioning (IC) systems aim to generate a text description of the salient objects in an image. In recent years, IC systems have been increasingly integrated into our daily lives, such as assistance for visually-impaired people and description generation in Microsoft Powerpoint. However, even the cutting-edge IC systems (e.g., Microsoft Azure Cognitive Services) and algorithms (e.g., OFA) could produce erroneous captions, leading to incorrect captioning of important objects, misunderstanding, and threats to personal safety. The existing testing approaches either fail to handle the complex form of IC system output (i.e., sentences in natural language) or generate unnatural images as test cases. To address these problems, we introduce Recursive Object MElting (Rome), a novel metamorphic testing approach for validating IC systems. Different from existing approaches that generate test cases by inserting objects, which easily make the generated images unnatural, Rome melts (i.e., remove and inpaint) objects. Rome assumes that the object set in the caption of an image includes the object set in the caption of a generated image after object melting. Given an image, Rome can recursively remove its objects to generate different pairs of images. We use Rome to test one widely-adopted image captioning API and four state-of-the-art (SOTA) algorithms. The results show that the test cases generated by Rome look much more natural than the SOTA IC testing approach and they achieve comparable naturalness to the original images. Meanwhile, by generating test pairs using 226 seed images, Rome reports a total of 9,121 erroneous issues with high precision (86.47%-92.17%). In addition, we further utilize the test cases generated by Rome to retrain the Oscar, which improves its performance across multiple evaluation metrics.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1145/3597926.3598094",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/issta/YuZLYHH23,\n  author       = {Boxi Yu and\n                  Zhiqing Zhong and\n                  Jiaqi Li and\n                  Yixing Yang and\n                  Shilin He and\n                  Pinjia He},\n  editor       = {Ren{\\'{e}} Just and\n                  Gordon Fraser},\n  title        = {{ROME:} Testing Image Captioning Systems via Recursive Object Melting},\n  booktitle    = {Proceedings of the 32nd {ACM} {SIGSOFT} International Symposium on\n                  Software Testing and Analysis, {ISSTA} 2023, Seattle, WA, USA, July\n                  17-21, 2023},\n  pages        = {766--778},\n  publisher    = {{ACM}},\n  year         = {2023},\n  url          = {https://doi.org/10.1145/3597926.3598094},\n  doi          = {10.1145/3597926.3598094},\n  timestamp    = {Mon, 03 Mar 2025 21:15:52 +0100},\n  biburl       = {https://dblp.org/rec/conf/issta/YuZLYHH23.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "SMART: A High-Performance Adaptive Radix Tree for Disaggregated Memory",
    "date": "2023",
    "authors": [
      "Xuchuan Luo",
      "Pengfei Zuo",
      "Jiacheng Shen",
      "Jiazhen Gu",
      "Xin Wang",
      "Michael R. Lyu",
      "Yangfan Zhou"
    ],
    "venue": "17th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2023, Boston, MA, USA, July 10-12, 2023",
    "venueShort": "OSDI",
    "tags": [],
    "abstract": "Disaggregated memory (DM) is an increasingly prevalent architecture in academia and industry with high resource utilization. It separates computing and memory resources into two pools and interconnects them with fast networks. Existing range indexes on DM are based on B+ trees, which suffer from large inherent read and write amplifications. The read and write amplifications rapidly saturate the network bandwidth, resulting in low request throughput and high access latency of B+ trees on DM. In this paper, we propose to use the radix tree, which is more suitable for DM than the B+ tree due to smaller read and write amplifications. However, constructing a radix tree on DM is challenging due to the costly lock-based concurrency control, the bounded memory-side IOPS, and the complicated computing-side cache validation. To address these challenges, we design SMART, the first radix tree for disaggregated memory with high performance. Specifically, we leverage 1) a hybrid concurrency control scheme including lock-free internal nodes and fine-grained lock-based leaf nodes to reduce lock overhead, 2) a computing-side read-delegation and write-combining technique to break through the IOPS upper bound by reducing redundant I/Os, and 3) a simple yet effective reverse check mechanism for computing-side cache validation. Experimental results show that SMART achieves 6.1x higher throughput under typical write-intensive workloads and 2.8x higher throughput under read-only workloads, compared with state-of-the-art B+ trees on DM.",
    "projectUrl": null,
    "paperUrl": "https://www.usenix.org/conference/osdi23/presentation/luo",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/osdi/LuoZSG0LZ23,\n  author       = {Xuchuan Luo and\n                  Pengfei Zuo and\n                  Jiacheng Shen and\n                  Jiazhen Gu and\n                  Xin Wang and\n                  Michael R. Lyu and\n                  Yangfan Zhou},\n  editor       = {Roxana Geambasu and\n                  Ed Nightingale},\n  title        = {{SMART:} {A} High-Performance Adaptive Radix Tree for Disaggregated\n                  Memory},\n  booktitle    = {17th {USENIX} Symposium on Operating Systems Design and Implementation,\n                  {OSDI} 2023, Boston, MA, USA, July 10-12, 2023},\n  pages        = {553--571},\n  publisher    = {{USENIX} Association},\n  year         = {2023},\n  url          = {https://www.usenix.org/conference/osdi23/presentation/luo},\n  timestamp    = {Tue, 11 Nov 2025 08:43:30 +0100},\n  biburl       = {https://dblp.org/rec/conf/osdi/LuoZSG0LZ23.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Towards Usable Neural Comment Generation via Code-Comment Linkage Interpretation: Method and Empirical Study",
    "date": "2023",
    "authors": [
      "Shuyao Jiang",
      "Jiacheng Shen",
      "Shengnan Wu",
      "Yu Cai",
      "Yue Yu",
      "Yangfan Zhou"
    ],
    "venue": "IEEE Trans. Software Eng.",
    "venueShort": "TSE",
    "tags": [],
    "abstract": "Code comment is important to facilitate code comprehension for developers. Recent studies suggest to generate comments automatically with deep learning, in particular, based on neural machine translation models. However, such a promising Neural Comment Generation (NCG) technique suffers from unsatisfactory performance, as well as poor usability, i.e., developers cannot easily understand and modify the auto-generated comments. This paper suggests that a proper interpretation of how the comments are generated can significantly improve the usability of NCG approaches. We propose a novel model-independent framework, namely CCLink, to interpret the auto-generated comments. CCLink generates a set of code mutants and obtains their corresponding comments. Based on these data, several contribution mining algorithms are designed to infer the key elements in code that contributes to the generation of the key phrases in the comments. The links between code and its auto-generated comment can thus be constructed. This in turn allows CCLink to visualize the links as the comment interpretations to developers. It greatly facilitates manual verification and correction of the comments. We examine the performance of CCLink with different contribution mining algorithms, NCG approaches, and real-world datasets. We also conduct an empirical study on 32 experienced Java programmers to evaluate the effectiveness of CCLink. The results show that CCLink is promising in making NCG more usable with a proper interpretation of the auto-generated comments.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1109/TSE.2022.3214859",
    "slidesUrl": null,
    "bibtex": "@article{DBLP:journals/tse/JiangSWCYZ23,\n  author       = {Shuyao Jiang and\n                  Jiacheng Shen and\n                  Shengnan Wu and\n                  Yu Cai and\n                  Yue Yu and\n                  Yangfan Zhou},\n  title        = {Towards Usable Neural Comment Generation via Code-Comment Linkage\n                  Interpretation: Method and Empirical Study},\n  journal      = {{IEEE} Trans. Software Eng.},\n  volume       = {49},\n  number       = {4},\n  pages        = {2239--2254},\n  year         = {2023},\n  url          = {https://doi.org/10.1109/TSE.2022.3214859},\n  doi          = {10.1109/TSE.2022.3214859},\n  timestamp    = {Mon, 10 Nov 2025 08:59:24 +0100},\n  biburl       = {https://dblp.org/rec/journals/tse/JiangSWCYZ23.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "A Survey on Automated Log Analysis for Reliability Engineering",
    "date": "2022",
    "authors": [
      "Shilin He",
      "Pinjia He",
      "Zhuangbin Chen",
      "Tianyi Yang",
      "Yuxin Su",
      "Michael R. Lyu"
    ],
    "venue": "ACM Comput. Surv.",
    "venueShort": "CSUR",
    "tags": [],
    "abstract": "Logs are semi-structured text generated by logging statements in software source code. In recent decades, software logs have become imperative in the reliability assurance mechanism of many software systems because they are often the only data available that record software runtime information. As modern software is evolving into a large scale, the volume of logs has increased rapidly. To enable effective and efficient usage of modern software logs in reliability engineering, a number of studies have been conducted on automated log analysis. This survey presents a detailed overview of automated log analysis research, including how to automate and assist the writing of logging statements, how to compress logs, how to parse logs into structured event templates, and how to employ logs to detect anomalies, predict failures, and facilitate diagnosis. Additionally, we survey work that releases open-source toolkits and datasets. Based on the discussion of the recent advances, we present several promising future directions toward real-world and next-generation automated log analysis.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1145/3460345",
    "slidesUrl": null,
    "bibtex": "@article{DBLP:journals/csur/HeHCYSL21,\n  author       = {Shilin He and\n                  Pinjia He and\n                  Zhuangbin Chen and\n                  Tianyi Yang and\n                  Yuxin Su and\n                  Michael R. Lyu},\n  title        = {A Survey on Automated Log Analysis for Reliability Engineering},\n  journal      = {{ACM} Comput. Surv.},\n  volume       = {54},\n  number       = {6},\n  pages        = {130:1--130:37},\n  year         = {2022},\n  url          = {https://doi.org/10.1145/3460345},\n  doi          = {10.1145/3460345},\n  timestamp    = {Sun, 19 Jan 2025 13:56:53 +0100},\n  biburl       = {https://dblp.org/rec/journals/csur/HeHCYSL21.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Characterizing and Mitigating Anti-patterns of Alerts in Industrial Cloud Systems",
    "date": "2022",
    "authors": [
      "Tianyi Yang",
      "Jiacheng Shen",
      "Yuxin Su",
      "Xiaoxue Ren",
      "Yongqiang Yang",
      "Michael R. Lyu"
    ],
    "venue": "52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2022, Baltimore, MD, USA, June 27-30, 2022",
    "venueShort": "DSN",
    "tags": [],
    "abstract": "Alerts are crucial for requesting prompt human intervention upon cloud anomalies. The quality of alerts significantly affects the cloud reliability and the cloud provider's business revenue. In practice, we observe on-call engineers being hindered from quickly locating and fixing faulty cloud services because of the vast existence of misleading, non-informative, non-actionable alerts. We call the ineffectiveness of alerts \"anti-patterns of alerts\". To better understand the anti-patterns of alerts and provide actionable measures to mitigate anti-patterns, in this paper, we conduct the first empirical study on the practices of mitigating anti-patterns of alerts in an industrial cloud system. We study the alert strategies and the alert processing procedure at Huawei Cloud, a leading cloud provider. Our study combines the quantitative analysis of millions of alerts in two years and a survey with eighteen experienced engineers. As a result, we summarized four individual anti-patterns and two collective anti-patterns of alerts. We also summarize four current reactions to mitigate the anti-patterns of alerts, and the general preventative guidelines for the configuration of alert strategy. Lastly, we propose to explore the automatic evaluation of the Quality of Alerts (QoA), including the indicativeness, precision, and handleability of alerts, as a future research direction that assists in the automatic detection of alerts' anti-patterns. The findings of our study are valuable for optimizing cloud monitoring systems and improving the reliability of cloud services.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1109/DSN53405.2022.00047",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/dsn/YangSSRYL22,\n  author       = {Tianyi Yang and\n                  Jiacheng Shen and\n                  Yuxin Su and\n                  Xiaoxue Ren and\n                  Yongqiang Yang and\n                  Michael R. Lyu},\n  title        = {Characterizing and Mitigating Anti-patterns of Alerts in Industrial\n                  Cloud Systems},\n  booktitle    = {52nd Annual {IEEE/IFIP} International Conference on Dependable Systems\n                  and Networks, {DSN} 2022, Baltimore, MD, USA, June 27-30, 2022},\n  pages        = {393--401},\n  publisher    = {{IEEE}},\n  year         = {2022},\n  url          = {https://doi.org/10.1109/DSN53405.2022.00047},\n  doi          = {10.1109/DSN53405.2022.00047},\n  timestamp    = {Sat, 30 Sep 2023 09:39:08 +0200},\n  biburl       = {https://dblp.org/rec/conf/dsn/YangSSRYL22.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Managing Service Dependency for Cloud Reliability: The Industrial Practice",
    "date": "2022",
    "authors": [
      "Tianyi Yang",
      "Baitong Li",
      "Jiacheng Shen",
      "Yuxin Su",
      "Yongqiang Yang",
      "Michael R. Lyu"
    ],
    "venue": "IEEE International Symposium on Software Reliability Engineering Workshops, ISSRE 2022 - Workshops, Charlotte, NC, USA, October 31 - Nov. 3, 2022",
    "venueShort": "ISSRE",
    "tags": [],
    "abstract": "Interactions between cloud services result in service dependencies. Evaluating and managing the cascading impacts caused by service dependencies is critical to the reliability of cloud systems. This paper summarizes the dependency types in cloud systems and demonstrates the design of the Dependency Management System (DMS), a platform for managing the service dependencies in the production cloud system. DMS features full-lifecycle support for service reliability (i.e., initial service deployment, service upgrade, proactive architectural optimization, and reactive failure mitigation) and refined characterization of the intensity of dependencies.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1109/ISSREW55968.2022.00041",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/issre/YangLS0YL22,\n  author       = {Tianyi Yang and\n                  Baitong Li and\n                  Jiacheng Shen and\n                  Yuxin Su and\n                  Yongqiang Yang and\n                  Michael R. Lyu},\n  title        = {Managing Service Dependency for Cloud Reliability: The Industrial\n                  Practice},\n  booktitle    = {{IEEE} International Symposium on Software Reliability Engineering\n                  Workshops, {ISSRE} 2022 - Workshops, Charlotte, NC, USA, October 31\n                  - Nov. 3, 2022},\n  pages        = {67--68},\n  publisher    = {{IEEE}},\n  year         = {2022},\n  url          = {https://doi.org/10.1109/ISSREW55968.2022.00041},\n  doi          = {10.1109/ISSREW55968.2022.00041},\n  timestamp    = {Sat, 30 Sep 2023 09:50:51 +0200},\n  biburl       = {https://dblp.org/rec/conf/issre/YangLS0YL22.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "AID: Efficient Prediction of Aggregated Intensity of Dependency in Large-scale Cloud Systems",
    "date": "2021",
    "authors": [
      "Tianyi Yang",
      "Jiacheng Shen",
      "Yuxin Su",
      "Xiao Ling",
      "Yongqiang Yang",
      "Michael R. Lyu"
    ],
    "venue": "36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021, Melbourne, Australia, November 15-19, 2021",
    "venueShort": "ASE",
    "tags": [],
    "abstract": "Service reliability is one of the key challenges that cloud providers have to deal with. In cloud systems, unplanned service failures may cause severe cascading impacts on their dependent services, deteriorating customer satisfaction. Predicting the cascading impacts accurately and efficiently is critical to the operation and maintenance of cloud systems. Existing approaches identify whether one service depends on another via distributed tracing but no prior work focused on discriminating to what extent the dependency between cloud services is. In this paper, we survey the outages and the procedure for failure diagnosis in two cloud providers to motivate the definition of the intensity of dependency. We define the intensity of dependency between two services as how much the status of the callee service influences the caller service. Then we propose AID, the first approach to predict the intensity of dependencies between cloud services. AID first generates a set of candidate dependency pairs from the spans. AID then represents the status of each cloud service with a multivariate time series aggregated from the spans. With the representation of services, AID calculates the similarities between the statuses of the caller and the callee of each candidate pair. Finally, AID aggregates the similarities to produce a unified value as the intensity of the dependency. We evaluate AID on the data collected from an open-source microservice benchmark and a cloud system in production. The experimental results show that AID can efficiently and accurately predict the intensity of dependencies. We further demonstrate the usefulness of our method in a large-scale commercial cloud system.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1109/ASE51524.2021.9678534",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/kbse/YangSSLYL21,\n  author       = {Tianyi Yang and\n                  Jiacheng Shen and\n                  Yuxin Su and\n                  Xiao Ling and\n                  Yongqiang Yang and\n                  Michael R. Lyu},\n  title        = {{AID:} Efficient Prediction of Aggregated Intensity of Dependency\n                  in Large-scale Cloud Systems},\n  booktitle    = {36th {IEEE/ACM} International Conference on Automated Software Engineering,\n                  {ASE} 2021, Melbourne, Australia, November 15-19, 2021},\n  pages        = {653--665},\n  publisher    = {{IEEE}},\n  year         = {2021},\n  url          = {https://doi.org/10.1109/ASE51524.2021.9678534},\n  doi          = {10.1109/ASE51524.2021.9678534},\n  timestamp    = {Sun, 19 Jan 2025 13:19:18 +0100},\n  biburl       = {https://dblp.org/rec/conf/kbse/YangSSLYL21.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Defuse: A Dependency-Guided Function Scheduler to Mitigate Cold Starts on FaaS Platforms",
    "date": "2021",
    "authors": [
      "Jiacheng Shen",
      "Tianyi Yang",
      "Yuxin Su",
      "Yangfan Zhou",
      "Michael R. Lyu"
    ],
    "venue": "41st IEEE International Conference on Distributed Computing Systems, ICDCS 2021, Washington DC, USA, July 7-10, 2021",
    "venueShort": "ICDCS",
    "tags": [],
    "abstract": "Function-as-a-Service (FaaS) is becoming a prevalent paradigm in developing cloud applications. With FaaS, clients can develop applications as serverless functions, leaving the burden of resource management to cloud providers. However, FaaS platforms suffer from the performance degradation caused by the cold starts of serverless functions. Cold starts happen when serverless functions are invoked before they have been loaded into the memory. The problem is unavoidable because the memory in datacenters is typically too limited to hold all serverless functions simultaneously. The latency of cold function invocations will greatly degenerate the performance of FaaS platforms. Currently, FaaS platforms employ various scheduling methods to reduce the occurrences of cold starts. However, they do not consider the ubiquitous dependencies between serverless functions. Observing the potential of using dependencies to mitigate cold starts, we propose Defuse, a Dependency-guided Function Scheduler on FaaS platforms. Specifically, Defuse identifies two types of dependencies between serverless functions, i.e., strong dependencies and weak ones. It uses frequent pattern mining and positive point-wise mutual information to mine such dependencies respectively from function invocation histories. In this way, Defuse constructs a function dependency graph. The connected components (i.e., dependent functions) on the graph can be scheduled to diminish the occurrences of cold starts. We evaluate the effectiveness of Defuse by applying it to an industrial serverless dataset. The experimental results show that Defuse can reduce 22% of memory usage while having a 35% decrease in function cold-start rates compared with the state-of-the-art method.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1109/ICDCS51616.2021.00027",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/icdcs/ShenYSZL21,\n  author       = {Jiacheng Shen and\n                  Tianyi Yang and\n                  Yuxin Su and\n                  Yangfan Zhou and\n                  Michael R. Lyu},\n  title        = {Defuse: {A} Dependency-Guided Function Scheduler to Mitigate Cold\n                  Starts on FaaS Platforms},\n  booktitle    = {41st {IEEE} International Conference on Distributed Computing Systems,\n                  {ICDCS} 2021, Washington DC, USA, July 7-10, 2021},\n  pages        = {194--204},\n  publisher    = {{IEEE}},\n  year         = {2021},\n  url          = {https://doi.org/10.1109/ICDCS51616.2021.00027},\n  doi          = {10.1109/ICDCS51616.2021.00027},\n  timestamp    = {Tue, 11 Nov 2025 08:43:33 +0100},\n  biburl       = {https://dblp.org/rec/conf/icdcs/ShenYSZL21.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Testing Machine Translation via Referential Transparency",
    "date": "2021",
    "authors": [
      "Pinjia He",
      "Clara Meister",
      "Zhendong Su"
    ],
    "venue": "43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021",
    "venueShort": "ICSE",
    "tags": [],
    "abstract": "Machine translation software has seen rapid progress in recent years due to the advancement of deep neural networks. People routinely use machine translation software in their daily lives, such as ordering food in a foreign restaurant, receiving medical diagnosis and treatment from foreign doctors, and reading international political news online. However, due to the complexity and intractability of the underlying neural networks, modern machine translation software is still far from robust and can produce poor or incorrect translations; this can lead to misunderstanding, financial loss, threats to personal safety and health, and political conflicts. To address this problem, we introduce referentially transparent inputs (RTIs), a simple, widely applicable methodology for validating machine translation software. A referentially transparent input is a piece of text that should have similar translations when used in different contexts. Our practical implementation, Purity, detects when this property is broken by a translation. To evaluate RTI, we use Purity to test Google Translate and Bing Microsoft Translator with 200 unlabeled sentences, which detected 123 and 142 erroneous translations with high precision (79.3% and 78.3%). The translation errors are diverse, including examples of under-translation, over-translation, word/phrase mistranslation, incorrect modification, and unclear logic.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1109/ICSE43902.2021.00047",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/icse/HeMS21,\n  author       = {Pinjia He and\n                  Clara Meister and\n                  Zhendong Su},\n  title        = {Testing Machine Translation via Referential Transparency},\n  booktitle    = {43rd {IEEE/ACM} International Conference on Software Engineering,\n                  {ICSE} 2021, Madrid, Spain, 22-30 May 2021},\n  pages        = {410--422},\n  publisher    = {{IEEE}},\n  year         = {2021},\n  url          = {https://doi.org/10.1109/ICSE43902.2021.00047},\n  doi          = {10.1109/ICSE43902.2021.00047},\n  timestamp    = {Tue, 30 Nov 2021 15:21:24 +0100},\n  biburl       = {https://dblp.org/rec/conf/icse/HeMS21.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Structure-invariant testing for machine translation",
    "date": "2020",
    "authors": [
      "Pinjia He",
      "Clara Meister",
      "Zhendong Su"
    ],
    "venue": "ICSE '20: 42nd International Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020",
    "venueShort": "ICSE",
    "tags": [],
    "abstract": "In recent years, machine translation software has increasingly been integrated into our daily lives. People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language. However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee. Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts. Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored.\n  To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software. Our key insight is that the translation results of \"similar\" source sentences should typically exhibit similar sentence structures. Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold. To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5\\% and 70\\% top-1 accuracy, respectively. The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1145/3377811.3380339",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/icse/HeMS20,\n  author       = {Pinjia He and\n                  Clara Meister and\n                  Zhendong Su},\n  editor       = {Gregg Rothermel and\n                  Doo{-}Hwan Bae},\n  title        = {Structure-invariant testing for machine translation},\n  booktitle    = {{ICSE} '20: 42nd International Conference on Software Engineering,\n                  Seoul, South Korea, 27 June - 19 July, 2020},\n  pages        = {961--973},\n  publisher    = {{ACM}},\n  year         = {2020},\n  url          = {https://doi.org/10.1145/3377811.3380339},\n  doi          = {10.1145/3377811.3380339},\n  timestamp    = {Tue, 30 Nov 2021 15:21:24 +0100},\n  biburl       = {https://dblp.org/rec/conf/icse/HeMS20.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Drain: An Online Log Parsing Approach with Fixed Depth Tree",
    "date": "2017",
    "authors": [
      "Pinjia He",
      "Jieming Zhu",
      "Zibin Zheng",
      "Michael R. Lyu"
    ],
    "venue": "2017 IEEE International Conference on Web Services, ICWS 2017, Honolulu, HI, USA, June 25-30, 2017",
    "venueShort": "ICWS",
    "tags": [],
    "abstract": "Logs, which record valuable system runtime information, have been widely employed in Web service management by service providers and users. A typical log analysis based Web service management procedure is to first parse raw log messages because of their unstructured format; and then apply data mining models to extract critical system behavior information, which can assist Web service management. Most of the existing log parsing methods focus on offline, batch processing of logs. However, as the volume of logs increases rapidly, model training of offline log parsing methods, which employs all existing logs after log collection, becomes time consuming. To address this problem, we propose an online log parsing method, namely Drain, that can parse logs in a streaming and timely manner. To accelerate the parsing process, Drain uses a fixed depth parse tree, which encodes specially designed rules for parsing. We evaluate Drain on five real-world log data sets with more than 10 million raw log messages. The experimental results show that Drain has the highest accuracy on four data sets, and comparable accuracy on the remaining one. Besides, Drain obtains 51.85%~81.47% improvement in running time compared with the state-of-the-art online parser. We also conduct a case study on an anomaly detection task using Drain in the parsing step, which determines the effectiveness of Drain in log analysis.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1109/ICWS.2017.13",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/icws/HeZZL17,\n  author       = {Pinjia He and\n                  Jieming Zhu and\n                  Zibin Zheng and\n                  Michael R. Lyu},\n  editor       = {Ilkay Altintas and\n                  Shiping Chen},\n  title        = {Drain: An Online Log Parsing Approach with Fixed Depth Tree},\n  booktitle    = {2017 {IEEE} International Conference on Web Services, {ICWS} 2017,\n                  Honolulu, HI, USA, June 25-30, 2017},\n  pages        = {33--40},\n  publisher    = {{IEEE}},\n  year         = {2017},\n  url          = {https://doi.org/10.1109/ICWS.2017.13},\n  doi          = {10.1109/ICWS.2017.13},\n  timestamp    = {Fri, 24 Mar 2023 00:02:31 +0100},\n  biburl       = {https://dblp.org/rec/conf/icws/HeZZL17.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  },
  {
    "title": "Experience Report: System Log Analysis for Anomaly Detection",
    "date": "2016",
    "authors": [
      "Shilin He",
      "Jieming Zhu",
      "Pinjia He",
      "Michael R. Lyu"
    ],
    "venue": "27th IEEE International Symposium on Software Reliability Engineering, ISSRE 2016, Ottawa, ON, Canada, October 23-27, 2016",
    "venueShort": "ISSRE",
    "tags": [],
    "abstract": "Anomaly detection plays an important role in management of modern large-scale distributed systems. Logs, which record system runtime information, are widely used for anomaly detection. Traditionally, developers (or operators) often inspect the logs manually with keyword search and rule matching. The increasing scale and complexity of modern systems, however, make the volume of logs explode, which renders the infeasibility of manual inspection. To reduce manual effort, many anomaly detection methods based on automated log analysis are proposed. However, developers may still have no idea which anomaly detection methods they should adopt, because there is a lack of a review and comparison among these anomaly detection methods. Moreover, even if developers decide to employ an anomaly detection method, re-implementation requires a nontrivial effort. To address these problems, we provide a detailed review and evaluation of six state-of-the-art log-based anomaly detection methods, including three supervised methods and three unsupervised methods, and also release an open-source toolkit allowing ease of reuse. These methods have been evaluated on two publicly-available production log datasets, with a total of 15,923,592 log messages and 365,298 anomaly instances. We believe that our work, with the evaluation results as well as the corresponding findings, can provide guidelines for adoption of these methods and provide references for future development.",
    "projectUrl": null,
    "paperUrl": "https://doi.org/10.1109/ISSRE.2016.21",
    "slidesUrl": null,
    "bibtex": "@inproceedings{DBLP:conf/issre/HeZHL16,\n  author       = {Shilin He and\n                  Jieming Zhu and\n                  Pinjia He and\n                  Michael R. Lyu},\n  title        = {Experience Report: System Log Analysis for Anomaly Detection},\n  booktitle    = {27th {IEEE} International Symposium on Software Reliability Engineering,\n                  {ISSRE} 2016, Ottawa, ON, Canada, October 23-27, 2016},\n  pages        = {207--218},\n  publisher    = {{IEEE} Computer Society},\n  year         = {2016},\n  url          = {https://doi.org/10.1109/ISSRE.2016.21},\n  doi          = {10.1109/ISSRE.2016.21},\n  timestamp    = {Fri, 24 Mar 2023 00:04:22 +0100},\n  biburl       = {https://dblp.org/rec/conf/issre/HeZHL16.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "arxivUrl": null,
    "awards": []
  }
]
